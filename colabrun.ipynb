{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaXYxzifbhst",
        "outputId": "3c5cb257-ad32-4567-d77f-76a871d69182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1y-ycbC7pCG3YRTPGQLNlg5hUbn0wSz28/volrt/git/Crossformer\n",
            "Your branch is up to date with 'origin/master'.\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ],
      "source": [
        "# \"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mydrive='/content/drive/MyDrive/volrt/'\n",
        "source_path=mydrive+\"git/Crossformer\"\n",
        "import os,sys\n",
        "if not os.path.exists(source_path):\n",
        "  %cd $mydrive/git\n",
        "  !git clone https://github.com/jerronl/Crossformer.git\n",
        "  %cd $source_path\n",
        "else:\n",
        "  %cd $source_path\n",
        "  !git checkout\n",
        "!pip install einops\n",
        "\n",
        "sys.path.append( source_path)\n",
        "# \"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from cross_exp.exp_crossformer import Exp_crossformer\n",
        "from utils.tools import string_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeTQzMtGy8Ih",
        "outputId": "b18c6db6-6c0f-4e0b-bda6-3de95867e04e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Args in experiment:\n",
            "Namespace(data='ETTh1', root_path='./datasets/', data_path='ETTh1.csv', data_split=[8640, 2880, 2880], checkpoints='./checkpoints/', in_len=168, out_len=24, seg_len=6, win_size=2, factor=10, data_dim=7, d_model=256, d_ff=512, n_heads=4, e_layers=3, dropout=0.2, baseline=False, num_workers=0, batch_size=32, train_epochs=20, patience=3, learning_rate=0.0001, lradj='type1', itr=5, save_pred=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3')\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : Crossformer_ETTh1_il168_ol24_sl6_win2_fa10_dm256_nh4_el3_itr0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 8449\n",
            "val 2857\n",
            "test 2857\n",
            "\titers: 100, epoch: 1 | loss: 0.4645257\n",
            "\tspeed: 0.0871s/iter; left time: 453.0299s\n",
            "\titers: 200, epoch: 1 | loss: 0.3246107\n",
            "\tspeed: 0.0762s/iter; left time: 388.9318s\n",
            "Epoch: 1 cost time: 21.77226233482361\n",
            "Epoch: 1, Steps: 265 | Train Loss: 0.4554227 Vali Loss: 0.5007090 Test Loss: 0.3459803\n",
            "Validation loss decreased (inf --> 0.500709).  Saving model ...\n",
            "\titers: 100, epoch: 2 | loss: 0.2376568\n",
            "\tspeed: 0.0812s/iter; left time: 401.0435s\n",
            "\titers: 200, epoch: 2 | loss: 0.2708249\n",
            "\tspeed: 0.0903s/iter; left time: 436.5402s\n",
            "Epoch: 2 cost time: 22.49112820625305\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='CrossFormer')\n",
        "\n",
        "parser.add_argument('--data', type=str,  default='ETTh1', help='data')\n",
        "parser.add_argument('--root_path', type=str, default='./datasets/', help='root path of the data file')\n",
        "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
        "parser.add_argument('--data_split', type=str, default='0.7,0.1,0.2',help='train/val/test split, can be ratio or number')\n",
        "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location to store model checkpoints')\n",
        "\n",
        "parser.add_argument('--in_len', type=int, default=168, help='input MTS length (T)')\n",
        "parser.add_argument('--out_len', type=int, default=24, help='output MTS length (\\tau)')\n",
        "parser.add_argument('--seg_len', type=int, default=6, help='segment length (L_seg)')\n",
        "parser.add_argument('--win_size', type=int, default=2, help='window size for segment merge')\n",
        "parser.add_argument('--factor', type=int, default=10, help='num of routers in Cross-Dimension Stage of TSA (c)')\n",
        "\n",
        "parser.add_argument('--data_dim', type=int, default=7, help='Number of dimensions of the MTS data (D)')\n",
        "parser.add_argument('--d_model', type=int, default=256, help='dimension of hidden states (d_model)')\n",
        "parser.add_argument('--d_ff', type=int, default=512, help='dimension of MLP in transformer')\n",
        "parser.add_argument('--n_heads', type=int, default=4, help='num of heads')\n",
        "parser.add_argument('--e_layers', type=int, default=3, help='num of encoder layers (N)')\n",
        "parser.add_argument('--dropout', type=float, default=0.2, help='dropout')\n",
        "\n",
        "parser.add_argument('--baseline', action='store_true', help='whether to use mean of past series as baseline for prediction', default=False)\n",
        "\n",
        "parser.add_argument('--num_workers', type=int, default=0, help='data loader num workers')\n",
        "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
        "parser.add_argument('--train_epochs', type=int, default=20, help='train epochs')\n",
        "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
        "parser.add_argument('--learning_rate', type=float, default=1e-4, help='optimizer initial learning rate')\n",
        "parser.add_argument('--lradj', type=str, default='type1',help='adjust learning rate')\n",
        "parser.add_argument('--itr', type=int, default=5, help='experiments times')\n",
        "\n",
        "parser.add_argument('--save_pred', action='store_true', help='whether to save the predicted future MTS', default=False)\n",
        "\n",
        "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
        "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
        "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
        "parser.add_argument('--devices', type=str, default='0,1,2,3',help='device ids of multile gpus')\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
        "\n",
        "if args.use_gpu and args.use_multi_gpu:\n",
        "    args.devices = args.devices.replace(' ','')\n",
        "    device_ids = args.devices.split(',')\n",
        "    args.device_ids = [int(id_) for id_ in device_ids]\n",
        "    args.gpu = args.device_ids[0]\n",
        "    print(args.gpu)\n",
        "\n",
        "data_parser = {\n",
        "    'ETTh1':{'data':'ETTh1.csv', 'data_dim':7, 'split':[12*30*24, 4*30*24, 4*30*24]},\n",
        "    'ETTm1':{'data':'ETTm1.csv', 'data_dim':7, 'split':[4*12*30*24, 4*4*30*24, 4*4*30*24]},\n",
        "    'WTH':{'data':'WTH.csv', 'data_dim':12, 'split':[28*30*24, 10*30*24, 10*30*24]},\n",
        "    'ECL':{'data':'ECL.csv', 'data_dim':321, 'split':[15*30*24, 3*30*24, 4*30*24]},\n",
        "    'ILI':{'data':'national_illness.csv', 'data_dim':7, 'split':[0.7, 0.1, 0.2]},\n",
        "    'Traffic':{'data':'traffic.csv', 'data_dim':862, 'split':[0.7, 0.1, 0.2]},\n",
        "}\n",
        "if args.data in data_parser.keys():\n",
        "    data_info = data_parser[args.data]\n",
        "    args.data_path = data_info['data']\n",
        "    args.data_dim = data_info['data_dim']\n",
        "    args.data_split = data_info['split']\n",
        "else:\n",
        "    args.data_split = string_split(args.data_split)\n",
        "\n",
        "print('Args in experiment:')\n",
        "print(args)\n",
        "\n",
        "Exp = Exp_crossformer\n",
        "\n",
        "for ii in range(args.itr):\n",
        "    # setting record of experiments\n",
        "    setting = 'Crossformer_{}_il{}_ol{}_sl{}_win{}_fa{}_dm{}_nh{}_el{}_itr{}'.format(args.data,\n",
        "                args.in_len, args.out_len, args.seg_len, args.win_size, args.factor,\n",
        "                args.d_model, args.n_heads, args.e_layers, ii)\n",
        "\n",
        "    exp = Exp(args) # set experiments\n",
        "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
        "    exp.train(setting)\n",
        "\n",
        "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "    exp.test(setting, args.save_pred)\n"
      ]
    }
  ]
}