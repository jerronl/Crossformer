{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KB0_RDrJTzT"
   },
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaXYxzifbhst"
   },
   "outputs": [],
   "source": [
    "run_test=True\n",
    "DO_TRAIN_VOLS = True\n",
    "DO_TRAIN_PRCS = True\n",
    "Provisional=True\n",
    "MODE = 'tuning'\n",
    "\n",
    "# run_test=False\n",
    "# DO_TRAIN_VOLS = False   # 是否运行 VOLS 的 VOL_GRID 训练\n",
    "# DO_TRAIN_PRCS = False  # 是否运行 PRCS 的 LMSE x VOL_GRID 交叉训练\n",
    "# Provisional=False\n",
    "# MODE = 'prod'\n",
    "\n",
    "\n",
    "epochs=(1,1,1,0,100) if Provisional else (15,500,3,10,100)\n",
    "\n",
    "cutdate='2025-11-18'\n",
    "valData=''\n",
    "use_amp=True\n",
    "use_amp=False\n",
    "# valData='2512'\n",
    "testData=''\n",
    "testData='new/'\n",
    "\n",
    "weight=(0.81,1.01,0.03,0)\n",
    "itr=5\n",
    "metrics_cnt=6\n",
    "import torch,sys\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  mydrive='/content/drive/MyDrive/volrt/'\n",
    "  checkpoints=mydrive+\"checkpoints/\"\n",
    "  source_path=mydrive+\"git/Crossformer\"\n",
    "  Provisional=Provisional and run_test\n",
    "  tables = [ 'volvTSLA.csv', 'volvAAPL.csv'] if Provisional else [ 'volvTSLA.csv', 'volvAAPL.csv', 'volvGOOG.csv','volvNVDA.csv', 'volvMETA.csv', 'volvMSFT.csv', 'volvAMZN.csv', ]\n",
    "  batch_size=int(torch.cuda.get_device_properties(0).total_memory/1e9+.5)*32\n",
    "  import os,sys\n",
    "  if not os.path.exists(source_path):\n",
    "    %cd $mydrive/git\n",
    "    !git clone https://github.com/jerronl/Crossformer.git\n",
    "    %cd $source_path\n",
    "  else:\n",
    "    %cd $source_path\n",
    "    # !git reset --hard HEAD\n",
    "    # !git pull origin master\n",
    "  %pip install einops #scikit-learn\n",
    "  sys.path.append( source_path)\n",
    "else:\n",
    "  tables = [ 'volvAAPL.csv']\n",
    "  testData=''\n",
    "  mydrive= 'E:/mydoc/git/trade/analyics/'\n",
    "  batch_size=32\n",
    "  sys.path.append(\"G:/git/Crossformer1/\")\n",
    "  itr=1\n",
    "  %load_ext autoreload\n",
    "  %autoreload 2\n",
    "%cd $mydrive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bmjLRHTfVt-"
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJovfe3GfVt-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# === Hyperparameter Grids (Dictionary Style) ===\n",
    "# Format: { \"arg_name\": ([list_of_values], in_vols_bool, in_prcs_bool) }\n",
    "PARAM_GRID = {\n",
    "    \"lambda_var\":  ([0.0004,0.0005],     True,  True),  # 基础方差权重\n",
    "    \"weight_over\": ([1.2, 1.5, 3.0],   True,  True),  # 非对称惩罚\n",
    "    \"dist_alpha\":  ([1.0, 3.0],   True,  True),  # 软标签锐度\n",
    "    \"lambda_mse\":  ([0.5, 1.0, 1.5],   False, True),  # 仅用于 Prcs\n",
    "    # 未来想加参数直接在这行添加，例如:\n",
    "    # \"dropout\":   ([0.1, 0.5],   True,  True),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwWfFDUcfVt_"
   },
   "source": [
    "# Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQsiHS9i8ua4"
   },
   "outputs": [],
   "source": [
    "from cross_exp.exp_crossformer import Exp_crossformer\n",
    "from data.data_loader import DatasetMTS, data_columns, data_names\n",
    "from data.data_def import set_cat\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import traceback\n",
    "import matplotlib.gridspec as gridspec\n",
    "from cross_exp.exp_crossformer import Exp_crossformer\n",
    "from data.data_loader import DatasetMTS, data_columns, data_names\n",
    "from data.data_def import set_cat\n",
    "from utils.tools import init_args, update_args\n",
    "\n",
    "set_cat(20)\n",
    "args = init_args()\n",
    "data_parser = {}\n",
    "\n",
    "COMMON_SETTINGS = {\n",
    "    \"e_layers\": 5,\n",
    "    \"d_model\": 512,\n",
    "    \"lradj\": \"type2\",\n",
    "    \"checkpoints\": checkpoints,\n",
    "}\n",
    "\n",
    "\n",
    "def generate_configs(grid, mode_idx):\n",
    "    active_keys = []\n",
    "    values_lists = []\n",
    "    for key, (vals, in_vols, in_prcs) in grid.items():\n",
    "        is_active = in_vols if mode_idx == 1 else in_prcs\n",
    "        if is_active:\n",
    "            active_keys.append(key)\n",
    "            values_lists.append(vals)\n",
    "    configs = []\n",
    "    for combination in itertools.product(*values_lists):\n",
    "        cfg = dict(zip(active_keys, combination))\n",
    "        configs.append(cfg)\n",
    "    return configs\n",
    "\n",
    "\n",
    "def apply_config_override(parser_config: dict, ds: str, config_dict: dict) -> dict:\n",
    "    if ds not in parser_config:\n",
    "        temp_config = parser_config.copy()\n",
    "    else:\n",
    "        temp_config = parser_config[ds].copy()\n",
    "    temp_config.update(config_dict)\n",
    "    if ds not in parser_config:\n",
    "        return temp_config\n",
    "    else:\n",
    "        parser_config[ds] = temp_config\n",
    "        return parser_config\n",
    "\n",
    "\n",
    "def build_parser(mode, ds, extra_query=\"\", root_override=None):\n",
    "    root = mydrive + valData if mode == \"insample\" else mydrive + testData\n",
    "    if root_override:\n",
    "        root = root_override\n",
    "    q = (\n",
    "        extra_query\n",
    "        if mode == \"insample\"\n",
    "        else (f\"date>'#{cutdate}'\" + (\" and \" + extra_query if extra_query else \"\"))\n",
    "    )\n",
    "    ds_config = COMMON_SETTINGS.copy()\n",
    "    ds_config[\"root_path\"] = root\n",
    "    ds_config[\"data_path\"] = tables\n",
    "    ds_config[\"weight\"] = weight[0] if ds == \"vols\" else weight[1]\n",
    "    ds_config[\"data\"] = ds\n",
    "    if q:\n",
    "        ds_config[\"query\"] = q\n",
    "    parser = {}\n",
    "    if ds != \"vols\":\n",
    "        parser[\"vols\"] = {\"checkpoints\": checkpoints}\n",
    "    parser[ds] = ds_config\n",
    "    return parser\n",
    "\n",
    "\n",
    "def build_train_parser(root_path=None):\n",
    "    if root_path == None:\n",
    "        root_path = mydrive + valData\n",
    "    base = {\n",
    "        \"patience\": epochs[0],\n",
    "        \"train_epochs\": TRAIN_EPOCHS,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"data_split\": [0.7, 0.15, 0.15],\n",
    "        \"batch_size\": batch_size * 2 // 5,\n",
    "        \"e_layers\": 5,\n",
    "        \"d_model\": 512,\n",
    "        \"lradj\": \"type2\",\n",
    "        \"checkpoints\": checkpoints,\n",
    "        \"root_path\": root_path,\n",
    "        \"data_path\": tables,\n",
    "        \"profile_mode\": False,\n",
    "        \"use_amp\": use_amp,\n",
    "    }\n",
    "    return {\n",
    "        \"vols\": {**base, \"data\": \"vols\", \"weight\": weight[0], \"over_weight\": weight[2]},\n",
    "        \"prcs\": {**base, \"data\": \"prcs\", \"weight\": weight[1], \"over_weight\": weight[3]},\n",
    "    }\n",
    "\n",
    "\n",
    "def get_feature_names(ds_name, in_len):\n",
    "    cols = data_columns(ds_name)\n",
    "    return data_names(cols, 1)[0]\n",
    "\n",
    "\n",
    "def get_trained_epochs(setting_str, ds):\n",
    "    folder_path = os.path.join(checkpoints, setting_str) + ds\n",
    "    file_path = os.path.join(folder_path, \"checkpoint.pth\")\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            state = torch.load(file_path, weights_only=False)\n",
    "            if isinstance(state, dict) and \"epoch\" in state:\n",
    "                return state[\"epoch\"]\n",
    "            else:\n",
    "                return \"N/A\"\n",
    "        except Exception:\n",
    "            return \"Err\"\n",
    "    else:\n",
    "        return \"Not Fnd\"\n",
    "\n",
    "\n",
    "def prepare_preds_trues_for_plot(preds, trues, ycat=0):\n",
    "    preds = np.asarray(preds)\n",
    "    trues = np.asarray(trues)\n",
    "    if ycat > 0:\n",
    "        reg_dim = preds.shape[1] - ycat\n",
    "        reg_preds = preds[:, :reg_dim]\n",
    "        if trues.shape[1] >= reg_dim:\n",
    "            reg_trues = trues[:, :reg_dim]\n",
    "        else:\n",
    "            min_dim = min(trues.shape[1], reg_dim)\n",
    "            reg_preds = reg_preds[:, :min_dim]\n",
    "            reg_trues = trues[:, :min_dim]\n",
    "        return reg_preds, reg_trues\n",
    "    return preds, trues\n",
    "\n",
    "\n",
    "def _metrics_text(t, p):\n",
    "    diff = p - t\n",
    "    mae = np.mean(np.abs(diff))\n",
    "    mse = np.mean(diff**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs(diff) / (np.abs(t) + 1e-12))\n",
    "    return f\"mae={mae:.4g}\\nrmse={rmse:.4g}\\nmape={mape:.3g}\"\n",
    "\n",
    "\n",
    "def _metrics_text_prcs_ic_iv(t_iv, p_iv, t_ic, p_ic_label):\n",
    "    diff = p_iv - t_iv\n",
    "    mae = np.mean(np.abs(diff))\n",
    "    mse = np.mean(diff**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs(diff) / (np.abs(t_iv) + 1e-12))\n",
    "    accr = np.nan\n",
    "    if len(t_ic) > 0 and (np.unique(t_ic).size <= 20) and (~np.isnan(t_ic)).sum() > 0:\n",
    "        if np.unique(p_ic_label).size <= 20:\n",
    "            accr = np.mean(\n",
    "                (p_ic_label.round() == t_ic.round())[\n",
    "                    ~np.isnan(p_ic_label) & ~np.isnan(t_ic)\n",
    "                ]\n",
    "            )\n",
    "    if not np.isnan(accr):\n",
    "        return f\"accr(IC)={accr:.3f}\\nmae(IV)={mae:.4g}\\nrmse(IV)={rmse:.4g}\"\n",
    "    else:\n",
    "        return f\"mae(IV)={mae:.4g}\\nrmse(IV)={rmse:.4g}\\nmape(IV)={mape:.3g}\"\n",
    "\n",
    "\n",
    "def _compute_limits_like_old_regplot(trues_list, preds_list, feat_idx, feat_name):\n",
    "    left, right = np.inf, -np.inf\n",
    "    for trues, preds in zip(trues_list, preds_list):\n",
    "        t = trues[:, feat_idx]\n",
    "        p = preds[:, feat_idx]\n",
    "        mask = ~np.isnan(t) & ~np.isnan(p)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        t, p = t[mask], p[mask]\n",
    "        if feat_name[:3] not in [\"dtm\", \"pmc\"]:\n",
    "            left = min(left, max(np.min(t), -5), max(np.min(p), -5))\n",
    "            right = max(right, min(np.max(t), 5), min(np.max(p), 5))\n",
    "        else:\n",
    "            left = min(left, np.min(t), np.min(p))\n",
    "            right = max(right, np.max(t), np.max(p))\n",
    "    if not np.isfinite(left) or not np.isfinite(right) or left == right:\n",
    "        left, right = -1, 1\n",
    "    return left, right\n",
    "\n",
    "\n",
    "def plot_residual_heatmap_family(\n",
    "    preds_raw,\n",
    "    trues_raw,\n",
    "    feature_names,\n",
    "    title,\n",
    "    bins=80,\n",
    "    ycat=0,\n",
    "    cols=4,\n",
    "    figsize_per_cell=4,\n",
    "):\n",
    "    preds, trues = prepare_preds_trues_for_plot(preds_raw, trues_raw, ycat=ycat)\n",
    "    out_dim = trues.shape[1]\n",
    "    max_plot_dim = len(feature_names)\n",
    "    effective_dim = min(out_dim, max_plot_dim)\n",
    "    rows = int(np.ceil(effective_dim / cols))\n",
    "    fig, axes = plt.subplots(\n",
    "        rows,\n",
    "        cols,\n",
    "        figsize=(figsize_per_cell * cols, figsize_per_cell * rows),\n",
    "        squeeze=False,\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "    for i in range(effective_dim):\n",
    "        r, c = divmod(i, cols)\n",
    "        ax = axes[r, c]\n",
    "        fname = feature_names[i]\n",
    "        t = trues[:, i]\n",
    "        p = preds[:, i]\n",
    "        mask = ~np.isnan(t) & ~np.isnan(p)\n",
    "        t, p = t[mask], p[mask]\n",
    "        if len(t) == 0:\n",
    "            ax.set_title(fname)\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        if ycat > 0 and i == 0:\n",
    "            p_cls = preds[mask, 4]\n",
    "            metrics_text = _metrics_text_prcs_ic_iv(t, p, t, p_cls)\n",
    "        else:\n",
    "            metrics_text = _metrics_text(t, p)\n",
    "        left, right = _compute_limits_like_old_regplot(\n",
    "            [trues_raw], [preds_raw], i, fname\n",
    "        )\n",
    "        ax.hist2d(t, p, bins=bins, range=[[left, right], [left, right]], norm=\"log\")\n",
    "        ax.plot([left, right], [left, right], \"w--\", alpha=0.45)\n",
    "        ax.set_xlim(left, right)\n",
    "        ax.set_ylim(left, right)\n",
    "        ax.set_xlabel(\"True\")\n",
    "        ax.set_ylabel(\"Pred\")\n",
    "        ax.set_title(fname, fontsize=12)\n",
    "        ax.text(\n",
    "            0.02,\n",
    "            0.98,\n",
    "            metrics_text,\n",
    "            transform=ax.transAxes,\n",
    "            va=\"top\",\n",
    "            ha=\"left\",\n",
    "            fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"black\", ec=\"none\", alpha=0.35),\n",
    "            color=\"white\",\n",
    "            zorder=3,\n",
    "        )\n",
    "    for i in range(effective_dim, rows * cols):\n",
    "        axes.flatten()[i].axis(\"off\")\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_multi_stock_residuals(\n",
    "    all_preds_raw,\n",
    "    all_trues_raw,\n",
    "    stock_labels,\n",
    "    feature_names,\n",
    "    ycat=0,\n",
    "    title=\"\",\n",
    "    bins=80,\n",
    "    cols=4,\n",
    "    figsize_per_cell=4,\n",
    "):\n",
    "    if not all_trues_raw:\n",
    "        return\n",
    "    out_dim = all_trues_raw[0].shape[1]\n",
    "    max_plot_dim = len(feature_names)\n",
    "    effective_dim = min(out_dim, max_plot_dim)\n",
    "    rows = int(np.ceil(effective_dim / cols))\n",
    "    fig = plt.figure(figsize=(figsize_per_cell * cols, figsize_per_cell * rows))\n",
    "    outer = gridspec.GridSpec(rows, cols, wspace=0.25, hspace=0.35)\n",
    "    cmap = plt.get_cmap(\"tab10\") if len(stock_labels) <= 10 else plt.get_cmap(\"tab20\")\n",
    "    colors = [cmap(i) for i in range(len(stock_labels))]\n",
    "    all_preds_reg, all_trues_reg = [], []\n",
    "    for preds_raw, trues_raw in zip(all_preds_raw, all_trues_raw):\n",
    "        p_reg, t_reg = prepare_preds_trues_for_plot(preds_raw, trues_raw, ycat=ycat)\n",
    "        all_preds_reg.append(p_reg)\n",
    "        all_trues_reg.append(t_reg)\n",
    "    for i in range(effective_dim):\n",
    "        r, c = divmod(i, cols)\n",
    "        ax_main = fig.add_subplot(outer[r, c])\n",
    "        ax_hist_bg = ax_main.twinx()\n",
    "        fname = feature_names[i]\n",
    "        combined_t, combined_p = all_trues_reg, all_preds_reg\n",
    "        left, right = _compute_limits_like_old_regplot(combined_t, combined_p, i, fname)\n",
    "        max_hist = 0\n",
    "        for j, (preds_reg, trues_reg) in enumerate(zip(all_preds_reg, all_trues_reg)):\n",
    "            t, p = trues_reg[:, i], preds_reg[:, i]\n",
    "            mask = ~np.isnan(t) & ~np.isnan(p)\n",
    "            t, p = t[mask], p[mask]\n",
    "            ax_main.scatter(\n",
    "                t,\n",
    "                p,\n",
    "                s=5,\n",
    "                alpha=0.4,\n",
    "                label=stock_labels[j],\n",
    "                color=colors[j % len(colors)],\n",
    "                zorder=2,\n",
    "            )\n",
    "            if len(t) > 1:\n",
    "                h, _, _ = ax_hist_bg.hist(\n",
    "                    t,\n",
    "                    bins=bins,\n",
    "                    density=True,\n",
    "                    alpha=0.15,\n",
    "                    color=colors[j % len(colors)],\n",
    "                    zorder=1,\n",
    "                )\n",
    "                max_hist = max(max_hist, np.max(h))\n",
    "        all_t = np.concatenate([t[:, i] for t in combined_t])\n",
    "        all_p = np.concatenate([p[:, i] for p in combined_p])\n",
    "        mask = ~np.isnan(all_t) & ~np.isnan(all_p)\n",
    "        t_all, p_all = all_t[mask], all_p[mask]\n",
    "        if len(t_all) == 0:\n",
    "            ax_main.set_title(fname)\n",
    "            ax_main.axis(\"off\")\n",
    "            continue\n",
    "        if ycat > 0 and i == 0:\n",
    "            all_p_cls = np.concatenate([p[:, 4] for p in combined_p])\n",
    "            p_cls_all = all_p_cls[mask]\n",
    "            metrics_text = _metrics_text_prcs_ic_iv(t_all, p_all, t_all, p_cls_all)\n",
    "        else:\n",
    "            metrics_text = _metrics_text(t_all, p_all)\n",
    "        ax_hist_bg.tick_params(\n",
    "            axis=\"y\", left=False, right=False, labelleft=False, labelright=False\n",
    "        )\n",
    "        ax_hist_bg.set_ylim(0, max_hist * 3.0 if max_hist > 0 else 1)\n",
    "        ax_hist_bg.set_ylabel(\"\")\n",
    "        ax_main.plot([left, right], [left, right], \"k--\", alpha=0.45)\n",
    "        ax_main.set_xlim(left, right)\n",
    "        ax_main.set_ylim(left, right)\n",
    "        ax_main.set_title(fname, fontsize=10)\n",
    "        ax_main.text(\n",
    "            0.02,\n",
    "            0.98,\n",
    "            metrics_text,\n",
    "            transform=ax_main.transAxes,\n",
    "            va=\"top\",\n",
    "            ha=\"left\",\n",
    "            fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"black\", ec=\"none\", alpha=0.35),\n",
    "            color=\"white\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        if i == effective_dim - 1:\n",
    "            ax_main.legend(fontsize=7, loc=\"lower right\", framealpha=0.5)\n",
    "    for i in range(effective_dim, rows * cols):\n",
    "        fig.add_subplot(outer[i]).axis(\"off\")\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_prcs_distribution_detailed(\n",
    "    results_list,\n",
    "    model_idx,\n",
    "    ycat,\n",
    "    dep_var,\n",
    "    tables_list,\n",
    "    cols=4,\n",
    "    title_prefix=\"PRCS Distribution\",\n",
    "):\n",
    "    if not results_list or ycat == 0:\n",
    "        print(\"No PRCS results or ycat=0, skipping detailed distribution plot.\")\n",
    "        return\n",
    "    n_categories = ycat\n",
    "    x = np.arange(1, n_categories + 1)\n",
    "    n_tables = len(results_list)\n",
    "    outer_rows = (n_tables - 1) // cols + 1\n",
    "    fig = plt.figure(figsize=(16, 4 * outer_rows))\n",
    "    outer = gridspec.GridSpec(outer_rows, cols, wspace=0.3, hspace=0.45)\n",
    "    ls = [\"-\", \"--\", \":\"]\n",
    "    print(f\"\\n--- Plotting {title_prefix} for Model m{model_idx} ---\")\n",
    "    for j, result in enumerate(results_list):\n",
    "        preds_raw, trues_raw, _ = result\n",
    "        preds = np.asarray(preds_raw)\n",
    "        trues = np.asarray(trues_raw)\n",
    "        probs = preds[:, -n_categories:]\n",
    "        true_labels = np.round(trues[:, 0]).astype(int)\n",
    "        valid_mask = (\n",
    "            (~np.isnan(true_labels))\n",
    "            & (true_labels >= 1)\n",
    "            & (true_labels <= n_categories)\n",
    "        )\n",
    "        if np.sum(valid_mask) == 0:\n",
    "            continue\n",
    "        probs = probs[valid_mask]\n",
    "        true_labels = true_labels[valid_mask]\n",
    "        if np.size(probs) > 0 and (np.max(probs) > 1.05 or np.min(probs) < -0.05):\n",
    "            exp_probs = np.exp(probs - np.max(probs, axis=1, keepdims=True))\n",
    "            probs = exp_probs / (np.sum(exp_probs, axis=1, keepdims=True) + 1e-12)\n",
    "        df = pd.DataFrame(probs, columns=[str(i) for i in range(1, n_categories + 1)])\n",
    "        df[\"True_Label\"] = true_labels\n",
    "        summed_probs = df.groupby(\"True_Label\").mean().T\n",
    "        xi = [int(x) for x in summed_probs.index]\n",
    "        third = n_categories // 3 + 1\n",
    "        r_outer = j // cols\n",
    "        c_outer = j % cols\n",
    "        sub_gs = outer[r_outer, c_outer].subgridspec(2, 1, hspace=0.0)\n",
    "        valid_true_labels = sorted(\n",
    "            [c for c in summed_probs.columns if 1 <= c <= n_categories]\n",
    "        )\n",
    "        for k in (0, 1):\n",
    "            ax = fig.add_subplot(sub_gs[k, 0])\n",
    "            hist_bins = np.arange(1, n_categories + 2) - 0.5\n",
    "            ax.hist(\n",
    "                true_labels,\n",
    "                bins=hist_bins,\n",
    "                alpha=0.2,\n",
    "                density=True,\n",
    "                color=\"gray\",\n",
    "                label=\"True\",\n",
    "                zorder=1,\n",
    "            )\n",
    "            ci = 0\n",
    "            for m in range(k, len(valid_true_labels), 2):\n",
    "                col = valid_true_labels[m]\n",
    "                ax.plot(\n",
    "                    xi,\n",
    "                    summed_probs[col],\n",
    "                    label=f\"cat {col}\",\n",
    "                    linestyle=ls[col // third % 3],\n",
    "                    color=f\"C{ci % 10}\",\n",
    "                    alpha=0.7,\n",
    "                    zorder=2,\n",
    "                )\n",
    "                pred_prob_at_true_class = summed_probs[col][str(col)]\n",
    "                ax.text(\n",
    "                    col,\n",
    "                    pred_prob_at_true_class,\n",
    "                    f\"{col}\",\n",
    "                    fontsize=8,\n",
    "                    va=\"bottom\",\n",
    "                    ha=\"center\",\n",
    "                    color=f\"C{(ci) % 10}\",\n",
    "                    zorder=3,\n",
    "                )\n",
    "                ci += 1\n",
    "            ax.set_title(\n",
    "                (\n",
    "                    f'{tables_list[j]} - {\"Odd\" if k == 0 else \"Even\"})'\n",
    "                    if k == 0\n",
    "                    else \"\"\n",
    "                ),\n",
    "                fontsize=10,\n",
    "            )\n",
    "            ax.set_ylim(0.0, 0.2)\n",
    "            ax.set_xlim(0.5, n_categories + 1.5)\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels([str(i) for i in x], rotation=-45, fontsize=8)\n",
    "            ax.legend(\n",
    "                loc=\"center left\",\n",
    "                bbox_to_anchor=(1.05, 0.5),\n",
    "                fontsize=7,\n",
    "                framealpha=0.6,\n",
    "            )\n",
    "            ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "            if k == 0:\n",
    "                plt.setp(ax.get_xticklabels(), visible=False)\n",
    "                ax.set_xlabel(\"\")\n",
    "            else:\n",
    "                ax.set_xlabel(\"Predicted Class Index\", fontsize=9)\n",
    "    fig.suptitle(f\"{title_prefix} - Model m{model_idx}\", fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_metric_summary(summary_records):\n",
    "    if not summary_records:\n",
    "        return\n",
    "    df = pd.DataFrame(summary_records)\n",
    "    metrics = [\"mae\", \"mse\", \"rmse\", \"mape\", \"accr\"]\n",
    "    metrics = [m for m in metrics if m in df.columns]\n",
    "    if not metrics:\n",
    "        return\n",
    "    cols = 2\n",
    "    rows = int(np.ceil(len(metrics) / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    axes = axes.flatten()\n",
    "    cmap = plt.get_cmap(\"tab10\")\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        if \"horizon\" in df.columns:\n",
    "            for idx_val in sorted(df[\"idx\"].unique()):\n",
    "                sub = df[df[\"idx\"] == idx_val].sort_values(\"horizon\")\n",
    "                ax.plot(\n",
    "                    sub[\"horizon\"],\n",
    "                    sub[metric],\n",
    "                    marker=\"o\",\n",
    "                    label=f\"Cfg {idx_val}\",\n",
    "                    linewidth=2,\n",
    "                )\n",
    "            ax.set_xlabel(\"Horizon\")\n",
    "        else:\n",
    "            ax.plot(df[\"idx\"], df[metric], marker=\"o\", linestyle=\"-\", color=cmap(i))\n",
    "            ax.set_xlabel(\"Config Index\")\n",
    "            ax.set_xticks(df[\"idx\"].unique())\n",
    "        ax.set_title(metric.upper())\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "        if \"horizon\" in df.columns:\n",
    "            ax.legend()\n",
    "    for i in range(len(metrics), len(axes)):\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.suptitle(\"Metric Comparison Summary\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_final_summary(summary_records):\n",
    "    if not summary_records:\n",
    "        print(\"No records to summarize.\")\n",
    "        return\n",
    "    df = pd.DataFrame(summary_records)\n",
    "    fixed_info_cols = [\"dataset\", \"idx\", \"horizon\", \"epochs\"]\n",
    "    metric_cols = [\"mae\", \"mse\", \"rmse\", \"mape\", \"mdst\", \"accr\"]\n",
    "    all_cols = df.columns.tolist()\n",
    "    param_cols = [\n",
    "        c for c in all_cols if c not in fixed_info_cols and c not in metric_cols\n",
    "    ]\n",
    "    final_order = []\n",
    "    for c in fixed_info_cols:\n",
    "        if c in df.columns:\n",
    "            final_order.append(c)\n",
    "    final_order += sorted(param_cols)\n",
    "    final_order += [c for c in metric_cols if c in df.columns]\n",
    "    print(df[final_order].to_string(float_format=lambda x: \"{:.4g}\".format(x)))\n",
    "\n",
    "\n",
    "def run_test_one_setting(\n",
    "    exp,\n",
    "    setting_str,\n",
    "    ds,\n",
    "    dep_var,\n",
    "    ycat,\n",
    "    title_prefix,\n",
    "    by_stock=False,\n",
    "    stock_tables=None,\n",
    "    plot_individual_heatmaps=True,\n",
    "):\n",
    "    metrics_list, all_preds, all_trues = [], [], []\n",
    "\n",
    "    def _plot_results(p_raw, t_raw, current_title):\n",
    "        if not plot_individual_heatmaps:\n",
    "            return\n",
    "        print(f\"\\n--- Residual Heatmap: {current_title} ---\")\n",
    "        plot_residual_heatmap_family(\n",
    "            p_raw, t_raw, dep_var, title=current_title, ycat=ycat\n",
    "        )\n",
    "\n",
    "    if by_stock:\n",
    "        for tb in stock_tables:\n",
    "            preds, trues, m = exp.test(\n",
    "                setting_str, ds, True, data_path=[tb], inverse=True\n",
    "            )\n",
    "            metrics_list.append(m)\n",
    "            all_preds.append(preds)\n",
    "            all_trues.append(trues)\n",
    "            _plot_results(preds, trues, f\"{title_prefix} {tb}\")\n",
    "        return metrics_list, all_preds, all_trues\n",
    "    else:\n",
    "        preds, trues, m = exp.test(setting_str, ds, True, inverse=True)\n",
    "        metrics_list.append(m)\n",
    "        _plot_results(preds, trues, title_prefix)\n",
    "        return metrics_list, None, None\n",
    "\n",
    "\n",
    "def train_unified(target_ds: str, best_idx: int = 0):\n",
    "    global args, data_parser, MODE, epochs\n",
    "    if target_ds == \"vols\":\n",
    "        configs = VOLS_CONFIGS\n",
    "    elif target_ds == \"prcs\":\n",
    "        configs = PRCS_CONFIGS\n",
    "    else:\n",
    "        return\n",
    "    base_parser = build_train_parser()\n",
    "    if not configs:\n",
    "        print(f\"No {target_ds.upper()} configs generated. Skipping training.\")\n",
    "        return\n",
    "    start_lr = base_parser[target_ds][\"learning_rate\"]\n",
    "    print(\n",
    "        f\"\\n=== Starting {target_ds.upper()} Training (Mode: {MODE}, Total configs: {len(configs)}) ===\"\n",
    "    )\n",
    "    if MODE == \"prod\":\n",
    "        if best_idx < 0 or best_idx >= len(configs):\n",
    "            print(f\"Error: Invalid best_idx ({best_idx}). Aborting PROD run.\")\n",
    "            return\n",
    "        cfg_idx = best_idx\n",
    "        for run_itr in range(args.itr):\n",
    "            if run_itr > 0:\n",
    "                DatasetMTS.clear()\n",
    "                print(f\"\\n--- Production Run {run_itr+1}/{args.itr} Started ---\")\n",
    "            cfg = configs[cfg_idx]\n",
    "            current_lr = start_lr\n",
    "            current_parser = {k: v.copy() for k, v in base_parser.items()}\n",
    "            current_parser[target_ds][\"learning_rate\"] = current_lr\n",
    "            current_parser[target_ds] = apply_config_override(\n",
    "                current_parser[target_ds], target_ds, cfg\n",
    "            )\n",
    "            setting_str = update_args(\n",
    "                args, current_parser, itr=run_itr, arg_set=target_ds\n",
    "            )\n",
    "            exp = Exp_crossformer(args)\n",
    "            cfg_display = \", \".join([f\"{k}={v}\" for k, v in cfg.items()])\n",
    "            print(f\"[{target_ds.upper()} | Run {run_itr+1}] Setting: {setting_str}\")\n",
    "            print(f\"   LR={current_lr:.6f} | Params: {{{cfg_display}}}\")\n",
    "            try:\n",
    "                exp.train(setting_str, target_ds)\n",
    "            except Exception as e:\n",
    "                print(f\"--- FAILED TRAINING {target_ds} index {cfg_idx} --- Error: {e}\")\n",
    "                traceback.print_exc()\n",
    "    elif MODE == \"tuning\":\n",
    "        num_cycles = epochs[2]\n",
    "        for cycle in range(num_cycles):\n",
    "            current_lr = start_lr / (3.0**cycle)\n",
    "            base_parser[target_ds][\"learning_rate\"] = current_lr\n",
    "            for idx in active_grid_indices:\n",
    "                if idx >= len(configs):\n",
    "                    break\n",
    "                cfg = configs[idx]\n",
    "                current_parser = {k: v.copy() for k, v in base_parser.items()}\n",
    "                current_parser[target_ds] = apply_config_override(\n",
    "                    current_parser[target_ds], target_ds, cfg\n",
    "                )\n",
    "                setting_str = update_args(\n",
    "                    args, current_parser, itr=0, arg_set=target_ds\n",
    "                )\n",
    "                exp = Exp_crossformer(args)\n",
    "                cfg_display = \", \".join([f\"{k}={v}\" for k, v in cfg.items()])\n",
    "                print(\n",
    "                    f\"[{target_ds.upper()} | Cyc {cycle+1} | Idx {idx}] Setting: {setting_str}\"\n",
    "                )\n",
    "                print(f\"   LR={current_lr:.6f} | Params: {{{cfg_display}}}\")\n",
    "                try:\n",
    "                    exp.train(setting_str, target_ds)\n",
    "                except Exception as e:\n",
    "                    print(f\"--- FAILED TRAINING {target_ds} index {idx} --- Error: {e}\")\n",
    "                    traceback.print_exc()\n",
    "\n",
    "\n",
    "def test_unified(\n",
    "    mode: str,\n",
    "    target_ds: str,\n",
    "    by_stock: bool = True,\n",
    "    plot_residuals: bool = True,\n",
    "    plot_stock: bool = False,\n",
    "):\n",
    "    global args\n",
    "    if target_ds == \"vols\":\n",
    "        configs = VOLS_CONFIGS\n",
    "        ycat = 0\n",
    "    elif target_ds == \"prcs\":\n",
    "        configs = PRCS_CONFIGS\n",
    "        ycat = data_columns(target_ds).get(\"ycat\", 0)\n",
    "    else:\n",
    "        return\n",
    "    dep_var = get_feature_names(target_ds, getattr(args, \"seq_len\", 1))\n",
    "    summary_data = []\n",
    "    if not configs:\n",
    "        return\n",
    "    print(\n",
    "        f\"\\n=== Starting {target_ds.upper()} Testing ({mode}) ({len(configs)} configs) ===\"\n",
    "    )\n",
    "    for idx in active_grid_indices:\n",
    "        if idx >= len(configs):\n",
    "            break\n",
    "        cfg = configs[idx]\n",
    "        base_parser = build_parser(mode, target_ds)\n",
    "        ds_config = apply_config_override(base_parser[target_ds].copy(), target_ds, cfg)\n",
    "        ds_config[\"data\"] = target_ds\n",
    "        base_parser[target_ds] = ds_config\n",
    "        setting_str = update_args(args, base_parser, itr=0, arg_set=target_ds)\n",
    "        trained_epochs = get_trained_epochs(setting_str, target_ds)\n",
    "        preds_fam = None\n",
    "        trues_fam = None\n",
    "        try:\n",
    "            exp = Exp_crossformer(args)\n",
    "            cfg_str = \", \".join([f\"{k}={v}\" for k, v in cfg.items()])\n",
    "            title = f\"{target_ds} {mode} [Idx {idx}] {cfg_str}\"\n",
    "            m_list, preds_fam, trues_fam = run_test_one_setting(\n",
    "                exp,\n",
    "                setting_str,\n",
    "                target_ds,\n",
    "                dep_var,\n",
    "                ycat,\n",
    "                title,\n",
    "                by_stock=by_stock,\n",
    "                stock_tables=tables,\n",
    "                plot_individual_heatmaps=plot_stock,\n",
    "            )\n",
    "            if by_stock and preds_fam is not None and plot_residuals:\n",
    "                print(f\"\\n--- AGGREGATED RESIDUAL PLOT: {title} ---\")\n",
    "                plot_multi_stock_residuals(\n",
    "                    preds_fam,\n",
    "                    trues_fam,\n",
    "                    tables,\n",
    "                    dep_var,\n",
    "                    ycat=ycat,\n",
    "                    title=f\"AGGREGATED {title}\",\n",
    "                )\n",
    "                if target_ds == \"prcs\" and ycat > 0:\n",
    "                    results_list = list(zip(preds_fam, trues_fam, m_list))\n",
    "                    print(f\"\\n--- DETAILED PRCS DISTRIBUTION: {title} ---\")\n",
    "                    plot_prcs_distribution_detailed(\n",
    "                        results_list=results_list,\n",
    "                        model_idx=idx,\n",
    "                        ycat=ycat,\n",
    "                        dep_var=dep_var,\n",
    "                        tables_list=tables,\n",
    "                        cols=4,\n",
    "                        title_prefix=f\"AGG {title}\",\n",
    "                    )\n",
    "            avg_metrics = np.mean(np.array(m_list), axis=0)\n",
    "            rec = {\n",
    "                \"dataset\": target_ds,\n",
    "                \"idx\": idx,\n",
    "                \"epochs\": trained_epochs,\n",
    "                **cfg,\n",
    "                \"mae\": avg_metrics[0],\n",
    "                \"mse\": avg_metrics[1],\n",
    "                \"rmse\": avg_metrics[2],\n",
    "                \"mape\": avg_metrics[3],\n",
    "                \"mdst\": avg_metrics[4],\n",
    "                \"accr\": avg_metrics[5],\n",
    "            }\n",
    "            summary_data.append(rec)\n",
    "        except Exception as e:\n",
    "            print(f\"--- FAILED TESTING {target_ds} idx {idx} --- Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "    generate_final_summary(summary_data)\n",
    "    if len(summary_data) > 1:\n",
    "        print(\"\\n--- Plotting Metric Comparisons ---\")\n",
    "        plot_metric_summary(summary_data)\n",
    "    return summary_data\n",
    "\n",
    "\n",
    "def test_vols_per_horizon(\n",
    "    mode: str, plot_residuals: bool = True, best_idx: int = 0, plot_stock: bool = False\n",
    "):\n",
    "    global args, VOLS_CONFIGS, MODE\n",
    "    ds = \"vols\"\n",
    "    horizons = list(range(5, 0, -1))\n",
    "    dep_var = get_feature_names(ds, getattr(args, \"seq_len\", 1))\n",
    "    summary_data = []\n",
    "    print(f\"\\n--- Running {mode} VOLS Per-Horizon Test (Horizons: {horizons}) ---\")\n",
    "    if MODE == \"prod\":\n",
    "        configs_to_run = [VOLS_CONFIGS[best_idx]]\n",
    "        indices_to_run = [best_idx]\n",
    "    else:\n",
    "        configs_to_run = VOLS_CONFIGS\n",
    "        indices_to_run = active_grid_indices\n",
    "    for p_idx in indices_to_run:\n",
    "        if p_idx >= len(configs_to_run):\n",
    "            break\n",
    "        cfg_idx = p_idx\n",
    "        cfg = configs_to_run[p_idx] if MODE == \"tuning\" else configs_to_run[0]\n",
    "        cfg_display = \", \".join([f\"{k}={v}\" for k, v in cfg.items()])\n",
    "        print(f\"\\n[VOLS Horizon Test | Idx {cfg_idx}] Params: {{{cfg_display}}}\")\n",
    "        for h_len in horizons:\n",
    "            base_parser = build_parser(mode, ds)\n",
    "            parser = {k: v.copy() for k, v in base_parser.items()}\n",
    "            parser[ds] = apply_config_override(parser[ds].copy(), ds, cfg)\n",
    "            parser[ds][\"data\"] = ds\n",
    "            hq = f\"floor(horizon)=={h_len}\"\n",
    "            existing_query = parser[ds].get(\"query\", \"\")\n",
    "            parser[ds][\"query\"] = (existing_query + \" and \" + hq).lstrip(\" and \")\n",
    "            setting_str = update_args(args, parser, 0, ds)\n",
    "            trained_epochs = get_trained_epochs(setting_str, ds)\n",
    "            try:\n",
    "                title_prefix = f\"VOLS {mode} H={h_len} [Idx {cfg_idx}]\"\n",
    "                exp = Exp_crossformer(args)\n",
    "                m_list, current_preds_fam, current_trues_fam = run_test_one_setting(\n",
    "                    exp,\n",
    "                    setting_str,\n",
    "                    ds,\n",
    "                    dep_var,\n",
    "                    ycat=0,\n",
    "                    title_prefix=title_prefix,\n",
    "                    by_stock=True,\n",
    "                    stock_tables=tables,\n",
    "                    plot_individual_heatmaps=plot_stock,\n",
    "                )\n",
    "                if m_list:\n",
    "                    avg_metrics = np.mean(np.array(m_list), axis=0)\n",
    "                    rec = {\n",
    "                        \"dataset\": ds,\n",
    "                        \"idx\": cfg_idx,\n",
    "                        \"horizon\": h_len,\n",
    "                        \"epochs\": trained_epochs,\n",
    "                        **cfg,\n",
    "                        \"mae\": avg_metrics[0],\n",
    "                        \"mse\": avg_metrics[1],\n",
    "                        \"rmse\": avg_metrics[2],\n",
    "                        \"mape\": avg_metrics[3],\n",
    "                        \"mdst\": avg_metrics[4],\n",
    "                        \"accr\": avg_metrics[5],\n",
    "                    }\n",
    "                    summary_data.append(rec)\n",
    "                if plot_residuals and current_preds_fam is not None:\n",
    "                    print(f\"\\n--- AGGREGATED RESIDUAL PLOT: {title_prefix} ---\")\n",
    "                    plot_multi_stock_residuals(\n",
    "                        current_preds_fam,\n",
    "                        current_trues_fam,\n",
    "                        tables,\n",
    "                        dep_var,\n",
    "                        ycat=0,\n",
    "                        title=f\"AGGREGATED {title_prefix}\",\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"--- FAILED Horizon Test H={h_len}, idx {cfg_idx} --- Error: {e}\"\n",
    "                )\n",
    "                traceback.print_exc()\n",
    "    generate_final_summary(summary_data)\n",
    "    if len(summary_data) > 1:\n",
    "        print(\"\\n--- Plotting Metric Comparisons (Horizon) ---\")\n",
    "        plot_metric_summary(summary_data)\n",
    "    return summary_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOLS_CONFIGS = generate_configs(PARAM_GRID, 1)\n",
    "PRCS_CONFIGS = generate_configs(PARAM_GRID, 2)\n",
    "\n",
    "if Provisional:\n",
    "    VOLS_CONFIGS = VOLS_CONFIGS[:2]\n",
    "    PRCS_CONFIGS = PRCS_CONFIGS[:2]\n",
    "\n",
    "total_configs = (len(VOLS_CONFIGS) if MODE == \"tuning\" else len(PRCS_CONFIGS))\n",
    "start_idx = epochs[3]\n",
    "end_idx = min(start_idx + epochs[4], total_configs)\n",
    "active_grid_indices = range(start_idx, end_idx)\n",
    "\n",
    "print(\"--- Config Preview (Active Slice) ---\")\n",
    "target_configs = VOLS_CONFIGS if len(VOLS_CONFIGS) > 0 else PRCS_CONFIGS\n",
    "for i in active_grid_indices:\n",
    "    if i < len(target_configs):\n",
    "        print(f\"  [{i}] {target_configs[i]}\")\n",
    "\n",
    "if MODE == \"tuning\":\n",
    "    TRAIN_EPOCHS = min(30, epochs[1])\n",
    "else:\n",
    "    TRAIN_EPOCHS = epochs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCfd-gU7JTzW"
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edfP8114fVuB"
   },
   "outputs": [],
   "source": [
    "if run_test == False:\n",
    "\n",
    "    if DO_TRAIN_VOLS:\n",
    "        print(\"\\n--- [Train VOLS with VOL_GRID] ---\")\n",
    "        train_unified('vols')\n",
    "\n",
    "    if DO_TRAIN_PRCS:\n",
    "        print(\"\\n--- [Train PRCS with LMSE x VOL_GRID] ---\")\n",
    "        train_unified('prcs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kj_rRG1J66H"
   },
   "outputs": [],
   "source": [
    "if not run_test:\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84_0x76B135B"
   },
   "outputs": [],
   "source": [
    "if not run_test:\n",
    "    from cross_exp.exp_crossformer import Exp_crossformer\n",
    "    from data.data_loader import DatasetMTS\n",
    "    data_parser = {\n",
    "        \"vols\": {\n",
    "            'data':'vols',\n",
    "            \"patience\":epochs[0],\n",
    "            \"train_epochs\":epochs[1],\n",
    "            'learning_rate':0.01,\n",
    "            'data_split':[0.7,0.15,0.15],\n",
    "            'batch_size':batch_size*2//5,\n",
    "            'e_layers':5,\n",
    "            'd_model':512,\n",
    "            'lradj':'type2',\n",
    "            \"checkpoints\":checkpoints,\n",
    "            'root_path':mydrive+valData,\n",
    "            'data_path':tables,\n",
    "            'weight':weight[0],\n",
    "            'over_weight':weight[2],\n",
    "            'profile_mode':False,\n",
    "            'use_amp':use_amp\n",
    "        },\n",
    "        \"prcs\":{\n",
    "            'data':'prcs',\n",
    "            'weight':weight[1],\n",
    "            'over_weight':weight[3],\n",
    "        }\n",
    "        }\n",
    "    s=epochs[3]\n",
    "    for _ in range(epochs[2]):\n",
    "        data_parser[\"vols\"][\"learning_rate\"]=.01\n",
    "        for i in range(epochs[2]):\n",
    "            for ii in range(s,itr):\n",
    "                # setting record of experiments\n",
    "                # data_parser[\"vols\"]['weight']=weight[0]\n",
    "                setting=update_args(args,data_parser,ii)\n",
    "                DatasetMTS.clear()\n",
    "                # drive.mount('/content/drive/',force_remount=True)\n",
    "                # !google-drive-ocamlfuse -cc # 注释掉这个bug行\n",
    "\n",
    "                exp = Exp_crossformer(args)  # set experiments\n",
    "                print(f\"start training :lr={data_parser[\"vols\"][\"learning_rate\"]} {setting}\")\n",
    "                exp.train(setting, \"vols\")\n",
    "\n",
    "                print(f\">>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "                preds, trues,_ = exp.test(setting, 'vols', True, data_path=[tables[-1]], inverse=True)\n",
    "                print(preds.shape, trues.shape)\n",
    "\n",
    "                # data_parser[\"vols\"]['weight']=weight[1]\n",
    "                setting=update_args(args,data_parser,ii,'prcs')\n",
    "                DatasetMTS.clear()\n",
    "                exp = Exp_crossformer(args)  # set experiments\n",
    "                print(f\"start training :lr={data_parser[\"vols\"][\"learning_rate\"]} {setting}\")\n",
    "                exp.train(setting, \"prcs\")\n",
    "                preds, trues,_ = exp.test(setting, 'prcs', True, data_path=[tables[-1]], inverse=True)\n",
    "                print(preds.shape, trues.shape)\n",
    "            data_parser[\"vols\"][\"learning_rate\"]/=3.\n",
    "            s=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "066vGmCNJTzY"
   },
   "source": [
    "# test\n",
    "##  PART A — In-sample (test split of same source)\n",
    "Subparts:\n",
    "1. vols per-stock heatmap  \n",
    "2. prcs per-stock heatmap + avg prob by true class  \n",
    "3. vols per-horizon heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w03KqC8jJTzY"
   },
   "outputs": [],
   "source": [
    "\n",
    "if run_test:\n",
    "            test_unified(\"insample\", \"vols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkGkKgp98ua9"
   },
   "source": [
    "## In-sample - Prcs Per-Stock\n",
    "\n",
    "Subparts:\n",
    "1. vols per-stock heatmap  \n",
    "2. prcs per-stock heatmap + avg prob by true class  \n",
    "3. vols per-horizon heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKW9afa-cENf"
   },
   "outputs": [],
   "source": [
    "if run_test:\n",
    "\n",
    "           test_unified(\"insample\", \"prcs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9IXNP9K8ua-"
   },
   "source": [
    "## In-sample - Horizon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpzR-1aN8ua-"
   },
   "outputs": [],
   "source": [
    "if run_test:\n",
    "        test_vols_per_horizon(mode=\"insample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBfRZp9y8ua-"
   },
   "source": [
    "# PART B — Out-of-sample (testData, cutdate+)\n",
    "## OOS - Vols Per-Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6WWmBSmcENg"
   },
   "outputs": [],
   "source": [
    "if run_test:\n",
    "\n",
    "        test_unified(\"oos\", \"vols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWJgEQeu8ua_"
   },
   "source": [
    "## hlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFlVJgjocENh"
   },
   "outputs": [],
   "source": [
    "if run_test:\n",
    "        test_unified(\"oos\", \"prcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff1dksSt8ubA"
   },
   "source": [
    "## horizon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPL62tkj8ubA"
   },
   "outputs": [],
   "source": [
    "if run_test:\n",
    "\n",
    "        test_vols_per_horizon(mode=\"oos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI4GPCiA8ubA"
   },
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45zLIW5xJTza"
   },
   "outputs": [],
   "source": [
    "plt.show()\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "s1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
