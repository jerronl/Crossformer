{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaXYxzifbhst",
        "outputId": "3c5cb257-ad32-4567-d77f-76a871d69182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E:\\mydoc\\git\\trade\\analyics\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  mydrive='/content/drive/MyDrive/volrt/'\n",
        "  source_path=mydrive+\"git/Crossformer\"\n",
        "  tables = ['volvN.csv', 'volvT.csv', 'volvA.csv', 'volvG.csv', ]\n",
        "  batch_size=int(torch.cuda.get_device_properties(0).total_memory/1e9+.5)*64\n",
        "  import os,sys\n",
        "  if not os.path.exists(source_path):\n",
        "    %cd $mydrive/git\n",
        "    !git clone https://github.com/jerronl/Crossformer.git\n",
        "    %cd $source_path\n",
        "  else:\n",
        "    %cd $source_path\n",
        "    !git reset --hard HEAD\n",
        "    !git pull origin master\n",
        "  !pip install einops #scikit-learn\n",
        "  sys.path.append( source_path)\n",
        "else:\n",
        "  tables = ['volvA.csv']\n",
        "  mydrive= 'E:/mydoc/git/trade/analyics/'\n",
        "  batch_size=32\n",
        "%cd $mydrive\n",
        "\n",
        "import argparse\n",
        "from utils.tools import string_split\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"CrossFormer\")\n",
        "\n",
        "parser.add_argument(\"--data\", type=str, default=\"vols\", help=\"data\")\n",
        "parser.add_argument(\n",
        "    \"--root_path\", type=str, default=mydrive, help=\"root path of the data file\"\n",
        ")\n",
        "parser.add_argument(\"--data_path\", type=list, default=tables, help=\"data file\")\n",
        "parser.add_argument(\n",
        "    \"--data_split\",\n",
        "    type=str,\n",
        "    default=\"0.7,0.1,0.2\",\n",
        "    help=\"train/val/test split, can be ratio or number\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--checkpoints\",\n",
        "    type=str,\n",
        "    default=\"./checkpoints/\",\n",
        "    help=\"location to store model checkpoints\",\n",
        ")\n",
        "\n",
        "parser.add_argument(\"--in_len\", type=int, default=20, help=\"input MTS length (T)\")\n",
        "parser.add_argument(\"--out_len\", type=int, default=1, help=\"output MTS length (\\tau)\")\n",
        "parser.add_argument(\"--seg_len\", type=int, default=5, help=\"segment length (L_seg)\")\n",
        "parser.add_argument(\n",
        "    \"--win_size\", type=int, default=2, help=\"window size for segment merge\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--factor\",\n",
        "    type=int,\n",
        "    default=10,\n",
        "    help=\"num of routers in Cross-Dimension Stage of TSA (c)\",\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--data_dim\", type=int, default=32, help=\"Number of dimensions of the MTS data (D)\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--out_dim\", type=int, default=3*4+4+22, help=\"Number of dimensions of the output\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--d_model\", type=int, default=256, help=\"dimension of hidden states (d_model)\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--d_ff\", type=int, default=512, help=\"dimension of MLP in transformer\"\n",
        ")\n",
        "parser.add_argument(\"--n_heads\", type=int, default=4, help=\"num of heads\")\n",
        "parser.add_argument(\"--e_layers\", type=int, default=3, help=\"num of encoder layers (N)\")\n",
        "parser.add_argument(\"--dropout\", type=float, default=0.2, help=\"dropout\")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--baseline\",\n",
        "    action=\"store_true\",\n",
        "    help=\"whether to use mean of past series as baseline for prediction\",\n",
        "    default=False,\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--num_workers\", type=int, default=0, help=\"data loader num workers\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--batch_size\", type=int, default=32, help=\"batch size of train input data\"\n",
        ")\n",
        "parser.add_argument(\"--train_epochs\", type=int, default=20, help=\"train epochs\")\n",
        "parser.add_argument(\"--patience\", type=int, default=3, help=\"early stopping patience\")\n",
        "parser.add_argument(\n",
        "    \"--learning_rate\", type=float, default=1e-4, help=\"optimizer initial learning rate\"\n",
        ")\n",
        "parser.add_argument(\"--lradj\", type=str, default=\"type1\", help=\"adjust learning rate\")\n",
        "parser.add_argument(\"--itr\", type=int, default=5, help=\"experiments times\")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--save_pred\",\n",
        "    action=\"store_true\",\n",
        "    help=\"whether to save the predicted future MTS\",\n",
        "    default=False,\n",
        ")\n",
        "\n",
        "parser.add_argument(\"--use_gpu\", type=bool, default=True, help=\"use gpu\")\n",
        "parser.add_argument(\"--resume\", type=bool, default=True, help=\"resume\")\n",
        "parser.add_argument(\"--cutday\", type=str, default=None, help=\"resume\")\n",
        "# parser.add_argument(\"--use_gpu\", type=bool, default=False, help=\"use gpu\")\n",
        "parser.add_argument(\"--gpu\", type=int, default=0, help=\"gpu\")\n",
        "parser.add_argument(\n",
        "    \"--use_multi_gpu\", action=\"store_true\", help=\"use multiple gpus\", default=False\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--devices\", type=str, default=\"0,1,2,3\", help=\"device ids of multile gpus\"\n",
        ")\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
        "\n",
        "if args.use_gpu and args.use_multi_gpu:\n",
        "    args.devices = args.devices.replace(\" \", \"\")\n",
        "    device_ids = args.devices.split(\",\")\n",
        "    args.device_ids = [int(id_) for id_ in device_ids]\n",
        "    args.gpu = args.device_ids[0]\n",
        "    print(args.gpu)\n",
        "\n",
        "def update_args(itr):\n",
        "    if args.data in data_parser.keys():\n",
        "        data_info = data_parser[args.data]\n",
        "        for k,v in data_info.items():\n",
        "            args.__setattr__(k,v)\n",
        "    if isinstance(args.data_split, str) :\n",
        "        args.data_split = string_split(args.data_split)\n",
        "\n",
        "    print(\"Args in experiment:\")\n",
        "    print(args)\n",
        "    setting = \"Crossformer_{}_il{}_ol{}_sl{}_win{}_fa{}_dm{}_nh{}_el{}_itr{}\".format(\n",
        "        args.data,\n",
        "        args.in_len,\n",
        "        args.out_len,\n",
        "        args.seg_len,\n",
        "        args.win_size,\n",
        "        args.factor,\n",
        "        args.d_model,\n",
        "        args.n_heads,\n",
        "        args.e_layers,\n",
        "        itr,\n",
        "    )\n",
        "    return setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeTQzMtGy8Ih",
        "outputId": "b18c6db6-6c0f-4e0b-bda6-3de95867e04e"
      },
      "outputs": [],
      "source": [
        "from cross_exp.exp_crossformer import Exp_crossformer\n",
        "data_parser = {\n",
        "    \"vols\": {\n",
        "        \"patience\":30,\n",
        "        \"train_epochs\":500,\n",
        "        'data_split':[0.7,0.1,0.2],\n",
        "        'batch_size':batch_size,\n",
        "    },\n",
        "    }\n",
        "\n",
        "for ii in range(1,args.itr):\n",
        "  setting=update_args(ii)\n",
        "  exp = Exp_crossformer(args)   # set experiments\n",
        "  print(f\">>>>>>>start training : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "  exp.train(setting)\n",
        "\n",
        "  print(f\">>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
        "  exp.test(setting, args.save_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Args in experiment:\n",
            "Namespace(data='vols', root_path='E:/mydoc/git/trade/analyics/', data_path=['volvA.csv'], data_split=[0.7, 0.1, 0.2], checkpoints='./checkpoints/', in_len=20, out_len=1, seg_len=5, win_size=2, factor=10, data_dim=32, out_dim=38, d_model=256, d_ff=512, n_heads=4, e_layers=3, dropout=0.2, baseline=False, num_workers=0, batch_size=32, train_epochs=20, patience=3, learning_rate=0.0001, lradj='type1', itr=5, save_pred=False, use_gpu=True, resume=True, cutday='#2024-02-01', gpu=0, use_multi_gpu=False, devices='0,1,2,3')\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>testing : Crossformer_vols_il20_ol1_sl5_win2_fa10_dm256_nh4_el3_itr0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "suc to load ./checkpoints/Crossformer_vols_il20_ol1_sl5_win2_fa10_dm256_nh4_el3_itr0/crossformer.pkl\n",
            "test 129\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Found array with dim 3. None expected <= 2.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m exp \u001b[38;5;241m=\u001b[39m Exp_crossformer(args) \n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>>>>>>testing : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m preds,trues\u001b[38;5;241m=\u001b[39mexp\u001b[38;5;241m.\u001b[39mtest(setting, \u001b[38;5;28;01mTrue\u001b[39;00m, data_path\u001b[38;5;241m=\u001b[39m[tables[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]], inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mg:\\git\\Crossformer1\\cross_exp\\exp_crossformer.py:253\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(self, setting, save_pred, inverse, data_path)\u001b[0m\n\u001b[0;32m    251\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    252\u001b[0m instance_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m--> 253\u001b[0m batch_metric \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(metric(pred, true)) \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[0;32m    254\u001b[0m metrics_all\u001b[38;5;241m.\u001b[39mappend(batch_metric)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_pred:\n",
            "File \u001b[1;32mg:\\git\\Crossformer1\\cross_exp\\exp_crossformer.py:295\u001b[0m, in \u001b[0;36m_process_one_batch\u001b[1;34m(self, dataset_object, batch_x, batch_y, inverse)\u001b[0m\n\u001b[0;32m    290\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch_x]\n\u001b[0;32m    291\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch_y[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtype(\n\u001b[0;32m    292\u001b[0m     torch\u001b[38;5;241m.\u001b[39mLongTensor\n\u001b[0;32m    293\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 295\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_x)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inverse:\n\u001b[0;32m    298\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m dataset_object\u001b[38;5;241m.\u001b[39minverse_transform(outputs)\n",
            "File \u001b[1;32mg:\\git\\Crossformer1\\data\\data_loader.py:195\u001b[0m, in \u001b[0;36mDatasetMTS.inverse_transform\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    193\u001b[0m dt \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    194\u001b[0m w\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mmean_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 195\u001b[0m dt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39minverse_transform(dt[:, \u001b[38;5;241m0\u001b[39m, :w]), dt[:, \u001b[38;5;241m0\u001b[39m, w:]),\n\u001b[0;32m    196\u001b[0m                     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (dt,data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m dt\n",
            "File \u001b[1;32md:\\ProgramData\\miniconda3\\envs\\e1\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1085\u001b[0m, in \u001b[0;36mStandardScaler.inverse_transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1082\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1084\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1085\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1086\u001b[0m     X,\n\u001b[0;32m   1087\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1088\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1089\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m   1090\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1091\u001b[0m )\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
            "File \u001b[1;32md:\\ProgramData\\miniconda3\\envs\\e1\\Lib\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    993\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m     )\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1000\u001b[0m     )\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m   1003\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1004\u001b[0m         array,\n\u001b[0;32m   1005\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1006\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1007\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1008\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."
          ]
        }
      ],
      "source": [
        "from cross_exp.exp_crossformer import Exp_crossformer\n",
        "data_parser = {\n",
        "    \"vols\": {\n",
        "        'batch_size':batch_size,\n",
        "        'cutday':'#2024-02-01',\n",
        "    },\n",
        "    }\n",
        "\n",
        "setting=update_args(0)\n",
        "exp = Exp_crossformer(args) \n",
        "print(f\">>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
        "preds,trues=exp.test(setting, True, data_path=[tables[-1]], inverse=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive E is data\n",
            " Volume Serial Number is DA6A-2572\n",
            "\n",
            " Directory of E:\\mydoc\\git\\trade\\analyics\\checkpoints\\Crossformer_vols_il20_ol1_sl5_win2_fa10_dm256_nh4_el3_itr0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "File Not Found\n"
          ]
        }
      ],
      "source": [
        "!dir \"./checkpoints/Crossformer_vols_il20_ol1_sl5_win2_fa10_dm256_nh4_el3_itr0/crossformer.pkl\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
